{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOVibRUzxlBRP8ZnjizSgD+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Theoretical**"],"metadata":{"id":"ximD5XQ2-5Zc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"i3gA7p0Z6hwM"},"outputs":[],"source":["### Q.1) What is a Support Vector Machine (SVM)?"]},{"cell_type":"code","source":["ans) A Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression tasks, though it is more commonly used for classification. SVM works by finding the optimal hyperplane that best separates different classes in a dataset."],"metadata":{"id":"pAyL5WO26kIl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.2) What is the difference between Hard Margin and Soft Margin SVM?"],"metadata":{"id":"ymZurhwL6kL6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans)  1) Hard Margin SVM: Requires perfect separation of classes, only works with linearly separable data.\n","2)  Soft Margin SVM: Allows some misclassifications, making it more robust for noisy or non-linearly separable data."],"metadata":{"id":"F5D-mXa86kPD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.3)  What is the mathematical intuition behind SVM?"],"metadata":{"id":"eGZYy-Rm6kR7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans)  SVM aims to maximize the margin between two classes while minimizing misclassification. It uses Lagrange multipliers and kernel functions to handle non-linearly separable data."],"metadata":{"id":"INGgZLNX6kVK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.4) What is the role of Lagrange Multipliers in SVM?"],"metadata":{"id":"asc8Mfi66kZC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans)  Lagrange Multipliers are used to transform the constrained optimization problem of SVM into an unconstrained dual problem, making it easier to solve using quadratic programming.\n","\n"],"metadata":{"id":"Rwh6347J6kdD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.5) What are Support Vectors in SVM?"],"metadata":{"id":"pAvj1bYa6khT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) Support vectors are the data points closest to the decision boundary. They determine the position of the optimal hyperplane."],"metadata":{"id":"tEEJtjKa6kli"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.6) What is a Support Vector Classifier (SVC)?"],"metadata":{"id":"YvPH89z66kpj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) SVC is the classification variant of SVM that assigns data points to predefined categories based on learned decision boundaries."],"metadata":{"id":"QlEbpRC06ktD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.7) What is a Support Vector Regressor (SVR)?"],"metadata":{"id":"bUdA0WWO6kw8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) SVR is the regression variant of SVM that finds a function that deviates from the true outputs by a small margin while minimizing error."],"metadata":{"id":"ppkXlXme6k0q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.8) What is the Kernel Trick in SVM?"],"metadata":{"id":"w6rKMLHw6k4b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) The kernel trick is a mathematical function that transforms input space into higher dimensions to make data linearly separable, without explicitly computing the transformation."],"metadata":{"id":"Jzk4DTVJ6k8c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.9) Compare Linear Kernel, Polynomial Kernel, and RBF Kernel."],"metadata":{"id":"JYZHFMdU6lAl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) 1) Linear Kernel: Works well for linearly separable data.\n","2) Polynomial Kernel: Captures complex relationships, using polynomial degrees to fit curved boundaries.\n","3) RBF Kernel: Maps data into an infinite-dimensional space, effective for non-linearly separable data."],"metadata":{"id":"OdN0XFIi6lFC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.10) What is the effect of the C parameter in SVM?"],"metadata":{"id":"GeymIIL76lIy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) The C parameter controls the trade-off between maximizing margin and minimizing classification error. A high C leads to less margin but fewer misclassifications, while a low C allows a larger margin with more misclassifications."],"metadata":{"id":"c3Pr_esR6lNF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.11) What is the role of the Gamma parameter in RBF Kernel SVM?"],"metadata":{"id":"IifKQJJd6lRq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) The Gamma parameter defines how far the influence of a single training example reaches. Higher gamma makes the model more complex, leading to overfitting."],"metadata":{"id":"hSLwAcYz6lUy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.12) What is the Naïve Bayes classifier, and why is it called \"Naïve\"?"],"metadata":{"id":"twTcOSlm6lYS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) Naïve Bayes is a probabilistic classification algorithm based on Bayes' Theorem. It is called \"naïve\" because it assumes that features are conditionally independent, which is often not true in real-world data."],"metadata":{"id":"tu2PN90w6lbz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.13) What is Bayes' Theorem?"],"metadata":{"id":"BrPi5jdw6lfT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans)  P(A|B) = [P(B|A) × P(A)] / P(B)\n","Where:\n","\n","P(A|B) is the posterior probability - the probability of A given that B occurred\n","P(B|A) is the likelihood - the probability of B given that A is true\n","P(A) is the prior probability of A\n","P(B) is the probability of B occurring"],"metadata":{"id":"jRfXuMOm6ljD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.14) Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes."],"metadata":{"id":"DUL_wkVT6lm7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) 1) Gaussian Naïve Bayes: Used for continuous numerical data, assumes a normal distribution.\n","2) Multinomial Naïve Bayes: Used for discrete data, effective for text classification (word counts).\n","3) Bernoulli Naïve Bayes: Works with binary features (0 or 1), useful for text classification with presence/absence of words."],"metadata":{"id":"xSrseP936lqz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.15) When should you use Gaussian Naïve Bayes over other variants?"],"metadata":{"id":"jkrlByuw6luc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) When working with continuous numerical data that follows a normal distribution.\n","\n"],"metadata":{"id":"zdTLPj3j6lx7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.16) What are the key assumptions made by Naïve Bayes?"],"metadata":{"id":"QfQUwJU66l1d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) 1) Features are independent given the class label.\n","2) Each feature contributes equally to the final prediction."],"metadata":{"id":"Ai6lzCzI6l5r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.17) What are the advantages and disadvantages of Naïve Bayes?"],"metadata":{"id":"k3tNEjkM6l9j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans)  Advantages: Simple, fast, works well with small datasets and high-dimensional data.\n","Disadvantages: Assumes feature independence, struggles with correlated features."],"metadata":{"id":"FCq39Uqv6mA7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.18) Why is Naïve Bayes a good choice for text classification?"],"metadata":{"id":"SOg9MU1D6mEq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) It handles high-dimensional data efficiently and performs well with sparse data, such as word frequencies in documents."],"metadata":{"id":"AZ1JTsAl6mIT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.19) Compare SVM and Naïve Bayes for classification tasks."],"metadata":{"id":"3qumw9_L6mMK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) 1) SVM: More accurate but computationally expensive, works well for small datasets.\n","2) Naïve Bayes: Faster and works well with large-scale text data but assumes feature independence."],"metadata":{"id":"EOb3LyBr6mQL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.20) How does Laplace Smoothing help in Naïve Bayes?"],"metadata":{"id":"S8yws2fa6mUb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) It prevents zero probabilities by adding a small constant to all probability estimates, making the model more robust for unseen words in text classification.\n","\n"],"metadata":{"id":"oeRxISnK6mYL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Practical**"],"metadata":{"id":"gqI4v1AO_LSZ"}},{"cell_type":"code","source":["### Q.21) Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy."],"metadata":{"id":"b4Rs0PAB6mcT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.metrics import classification_report\n","\n","# Load the Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Create and train the SVM Classifier\n","svm_classifier = SVC(kernel='rbf', random_state=42)\n","svm_classifier.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred = svm_classifier.predict(X_test)\n","\n","# Calculate and print accuracy\n","accuracy = svm_classifier.score(X_test, y_test)\n","print(f\"Accuracy: {accuracy:.2f}\")\n","\n","# Print classification report\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"],"metadata":{"id":"FDN36ao26mgD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.22)  Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n"],"metadata":{"id":"4p8lOMiK6mjj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) from sklearn import datasets, svm, metrics\n","from sklearn.model_selection import train_test_split\n","\n","# Load the Wine dataset\n","wine = datasets.load_wine()\n","X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.2, random_state=42)\n","\n","# Train SVM with Linear kernel\n","linear_svm = svm.SVC(kernel='linear')\n","linear_svm.fit(X_train, y_train)\n","\n","# Train SVM with RBF kernel\n","rbf_svm = svm.SVC(kernel='rbf')\n","rbf_svm.fit(X_train, y_train)\n","\n","# Evaluate and compare accuracies\n","print(\"Linear Kernel Accuracy:\", linear_svm.score(X_test, y_test))\n","print(\"RBF Kernel Accuracy:\", rbf_svm.score(X_test, y_test))\n"],"metadata":{"id":"YhDxEbdJ6mny"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.23) Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean Squared Error (MSE)."],"metadata":{"id":"4xL4DxUP6mra"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import SVR\n","from sklearn.metrics import mean_squared_error\n","\n","# Load the California housing dataset\n","housing = datasets.fetch_california_housing()\n","X, y = housing.data, housing.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize features by scaling to unit variance\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# Train the SVM Regressor with a radial basis function kernel\n","svr = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n","svr.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred = svr.predict(X_test)\n","\n","# Evaluate using Mean Squared Error (MSE)\n","mse = mean_squared_error(y_test, y_pred)\n","print(f'Mean Squared Error: {mse:.2f}')\n"],"metadata":{"id":"kwVc62gpCwi6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.24) Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision boundary.\n"],"metadata":{"id":"rT59kN256m3S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.svm import SVC\n","from sklearn.datasets import make_moons\n","from sklearn.preprocessing import StandardScaler\n","\n","# Generate sample data\n","X, y = make_moons(n_samples=100, noise=0.15, random_state=42)\n","\n","# Scale the features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Create and train the SVM with polynomial kernel\n","svm = SVC(kernel='poly', degree=3, C=1.0, random_state=42)\n","svm.fit(X_scaled, y)\n","\n","# Create a mesh grid to plot the decision boundary\n","x_min, x_max = X_scaled[:, 0].min() - 0.5, X_scaled[:, 0].max() + 0.5\n","y_min, y_max = X_scaled[:, 1].min() - 0.5, X_scaled[:, 1].max() + 0.5\n","xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n","                     np.arange(y_min, y_max, 0.02))\n","\n","# Make predictions on the mesh grid\n","Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n","Z = Z.reshape(xx.shape)\n","\n","# Plot the decision boundary and data points\n","plt.figure(figsize=(10, 8))\n","plt.contourf(xx, yy, Z, alpha=0.4)\n","plt.scatter(X_scaled[y == 0, 0], X_scaled[y == 0, 1], color='blue', label='Class 0')\n","plt.scatter(X_scaled[y == 1, 0], X_scaled[y == 1, 1], color='red', label='Class 1')\n","\n","# Plot support vectors\n","plt.scatter(svm.support_vectors_[:, 0], svm.support_vectors_[:, 1],\n","           s=100, linewidth=1, facecolors='none', edgecolors='black',\n","           label='Support Vectors')\n","\n","plt.title('SVM with Polynomial Kernel Decision Boundary')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.legend()\n","plt.grid(True)\n","\n","# Calculate and print accuracy\n","accuracy = svm.score(X_scaled, y)\n","print(f\"Model Accuracy: {accuracy:.2f}\")\n","\n","# Show number of support vectors\n","print(f\"Number of support vectors: {len(svm.support_vectors_)}\")\n","plt.show()\n"],"metadata":{"id":"kQb29mHN6m7D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.25) Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy."],"metadata":{"id":"IW6GfYqG6m-L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import accuracy_score\n","\n","# Load the Breast Cancer dataset\n","cancer = datasets.load_breast_cancer()\n","X, y = cancer.data, cancer.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize features by scaling to unit variance\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# Train the Gaussian Naïve Bayes classifier\n","nb_clf = GaussianNB()\n","nb_clf.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred = nb_clf.predict(X_test)\n","\n","# Evaluate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f'Accuracy: {accuracy:.2f}')\n"],"metadata":{"id":"UvZKdqvW6nBc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.26) Write a Python program to train a Multinomial Naïve Bayes classifier for text classification using the 20 Newsgroups dataset."],"metadata":{"id":"BYIZbLoZ6nFs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) from sklearn.datasets import fetch_20newsgroups\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.pipeline import Pipeline\n","import numpy as np\n","\n","# Load the 20 newsgroups dataset\n","# Using a subset of categories for simplicity\n","categories = ['alt.atheism', 'soc.religion.christian',\n","              'comp.graphics', 'sci.med']\n","newsgroups_train = fetch_20newsgroups(subset='train',\n","                                     categories=categories,\n","                                     remove=('headers', 'footers', 'quotes'))\n","newsgroups_test = fetch_20newsgroups(subset='test',\n","                                    categories=categories,\n","                                    remove=('headers', 'footers', 'quotes'))\n","\n","# Create a pipeline with TF-IDF vectorizer and Multinomial NB classifier\n","text_clf = Pipeline([\n","    ('tfidf', TfidfVectorizer(stop_words='english',\n","                             max_features=5000,\n","                             ngram_range=(1, 2))),\n","    ('clf', MultinomialNB(alpha=1.0))\n","])\n","\n","# Train the classifier\n","text_clf.fit(newsgroups_train.data, newsgroups_train.target)\n","\n","# Make predictions\n","predictions = text_clf.predict(newsgroups_test.data)\n","\n","# Calculate accuracy\n","accuracy = text_clf.score(newsgroups_test.data, newsgroups_test.target)\n","\n","# Print results\n","print(\"Text Classification Results\")\n","print(\"--------------------------\")\n","print(f\"Model Accuracy: {accuracy:.4f}\")\n","\n","print(\"\\nClassification Report:\")\n","print(classification_report(newsgroups_test.target, predictions,\n","                          target_names=categories))\n","\n","# Get feature names and their importance\n","tfidf = text_clf.named_steps['tfidf']\n","nb = text_clf.named_steps['clf']\n","\n","# Get top features for each category\n","def print_top_features(classifier, vectorizer, class_labels, n=5):\n","    feature_names = np.array(vectorizer.get_feature_names_out())\n","    for i, category in enumerate(class_labels):\n","        top_features_idx = np.argsort(classifier.feature_log_prob_[i])[-n:]\n","        top_features = feature_names[top_features_idx]\n","        print(f\"\\nTop {n} features for {category}:\")\n","        for feature in reversed(top_features):\n","            print(f\"- {feature}\")\n","\n","print(\"\\nMost Informative Features per Category:\")\n","print_top_features(nb, tfidf, categories)\n","\n","# Example of prediction with new text\n","def predict_category(text):\n","    prediction = text_clf.predict([text])\n","    probability = text_clf.predict_proba([text])[0]\n","    return categories[prediction[0]], probability[prediction[0]]\n","\n","# Test with a sample text\n","sample_text = \"The patient was prescribed antibiotics for the infection\"\n","category, confidence = predict_category(sample_text)\n","print(f\"\\nSample Text Classification:\")\n","print(f\"Text: {sample_text}\")\n","print(f\"Predicted Category: {category}\")\n","print(f\"Confidence: {confidence:.4f}\")"],"metadata":{"id":"Z6r3yeo3BGrz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.27) Write a Python program to train an SVM Classifier with different C values and compare the decision boundaries visually."],"metadata":{"id":"x8EON26qBG27"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import SVC\n","\n","# Load the Iris dataset\n","iris = datasets.load_iris()\n","X, y = iris.data[:, :2]  # Use only the first two features for visualization\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize features by scaling to unit variance\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# Different values of C to compare\n","C_values = [0.1, 1, 10, 100]\n","\n","fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n","axes = axes.ravel()\n","\n","for i, C in enumerate(C_values):\n","    svm_clf = SVC(kernel='linear', C=C, random_state=42)\n","    svm_clf.fit(X_train, y_train)\n","\n","    # Create a mesh grid for visualization\n","    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n","\n","    # Predict for each point in the mesh grid\n","    Z = svm_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n","    Z = Z.reshape(xx.shape)\n","\n","    # Plot the decision boundary and data points\n","    axes[i].contourf(xx, yy, Z, alpha=0.3)\n","    axes[i].scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', cmap=plt.cm.jet)\n","    axes[i].set_xlabel('Feature 1')\n","    axes[i].set_ylabel('Feature 2')\n","    axes[i].set_title(f'SVM with C={C}')\n","\n","plt.tight_layout()\n","plt.show()\n","\n"],"metadata":{"id":"fqJW7caxBHBD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.28)  Write a Python program to train a Bernoulli Naïve Bayes classifier for binary classification on a dataset with binary features."],"metadata":{"id":"135xaTfEBHFr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) from sklearn.naive_bayes import BernoulliNB\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report\n","import numpy as np\n","\n","# Generate a sample dataset with binary features\n","np.random.seed(0)\n","X = np.random.randint(2, size=(100, 10))  # 100 samples, 10 binary features\n","y = np.random.randint(2, size=(100,))  # Binary target variable\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train a Bernoulli Naive Bayes classifier\n","bnb = BernoulliNB()\n","bnb.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","y_pred = bnb.predict(X_test)\n","\n","# Evaluate the classifier's performance\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Accuracy:\", accuracy)\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred))\n","\n"],"metadata":{"id":"hb6UURAFBHJ7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.29) Apply feature scaling before training an SVM model and compare results with unscaled data."],"metadata":{"id":"jsrgBKaBBHN8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","\n","# Load the Iris dataset\n","iris = datasets.load_iris()\n","X, y = iris.data[:, :2]  # Use only the first two features for visualization\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train SVM without feature scaling\n","svm_clf_unscaled = SVC(kernel='linear', C=1.0, random_state=42)\n","svm_clf_unscaled.fit(X_train, y_train)\n","y_pred_unscaled = svm_clf_unscaled.predict(X_test)\n","unscaled_accuracy = accuracy_score(y_test, y_pred_unscaled)\n","\n","# Apply feature scaling\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Train SVM with feature scaling\n","svm_clf_scaled = SVC(kernel='linear', C=1.0, random_state=42)\n","svm_clf_scaled.fit(X_train_scaled, y_train)\n","y_pred_scaled = svm_clf_scaled.predict(X_test_scaled)\n","scaled_accuracy = accuracy_score(y_test, y_pred_scaled)\n","\n","# Print accuracy comparison\n","print(f'Accuracy without scaling: {unscaled_accuracy:.2f}')\n","print(f'Accuracy with scaling: {scaled_accuracy:.2f}')\n","\n","# Plot decision boundaries\n","fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n","for i, (clf, title, X_train_plot, X_test_plot) in enumerate([\n","    (svm_clf_unscaled, 'Without Scaling', X_train, X_test),\n","    (svm_clf_scaled, 'With Scaling', X_train_scaled, X_test_scaled)\n","]):\n","    x_min, x_max = X_train_plot[:, 0].min() - 1, X_train_plot[:, 0].max() + 1\n","    y_min, y_max = X_train_plot[:, 1].min() - 1, X_train_plot[:, 1].max() + 1\n","    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n","    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n","    Z = Z.reshape(xx.shape)\n","\n","    axes[i].contourf(xx, yy, Z, alpha=0.3)\n","    axes[i].scatter(X_train_plot[:, 0], X_train_plot[:, 1], c=y_train, edgecolor='k', cmap=plt.cm.jet)\n","    axes[i].set_xlabel('Feature 1')\n","    axes[i].set_ylabel('Feature 2')\n","    axes[i].set_title(title)\n","\n","plt.tight_layout()\n","plt.show()\n","\n"],"metadata":{"id":"YGYN44psBHUr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.30) Train a Gaussian Naïve Bayes model and compare predictions before and after Laplace Smoothing.\n","\n"],"metadata":{"id":"NGhJO1SHBHYL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) from sklearn.naive_bayes import GaussianNB\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report\n","import numpy as np\n","\n","# Generate a sample dataset\n","X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=3, n_repeated=2, random_state=42)\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train a Gaussian Naive Bayes model without Laplace Smoothing\n","gnb = GaussianNB()\n","gnb.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","y_pred = gnb.predict(X_test)\n","\n","# Evaluate the model's performance\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Accuracy without Laplace Smoothing:\", accuracy)\n","print(\"Classification Report without Laplace Smoothing:\")\n","print(classification_report(y_test, y_pred))\n","\n","# Apply Laplace Smoothing\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.preprocessing import Normalizer\n","normalizer = Normalizer()\n","X_train_normalized = normalizer.fit_transform(X_train)\n","X_test_normalized = normalizer.transform(X_test)\n","\n","mnb = MultinomialNB(alpha=1.0e-10) # alpha is the smoothing parameter\n","mnb.fit(X_train_normalized, y_train)\n","\n","# Make predictions on the test set\n","y_pred_smoothed = mnb.predict(X_test_normalized)\n","\n","# Evaluate the model's performance\n","accuracy_smoothed = accuracy_score(y_test, y_pred_smoothed)\n","print(\"Accuracy with Laplace Smoothing:\", accuracy_smoothed)\n","print(\"Classification Report with Laplace Smoothing:\")\n","print(classification_report(y_test, y_pred_smoothed))\n","\n"],"metadata":{"id":"QH1kFmbZBHbi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.31) Train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C, gamma, kernel).\n","\n"],"metadata":{"id":"YSDsBIReBHfL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn import svm\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# Load the iris dataset\n","iris = datasets.load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Define the hyperparameter tuning space\n","param_grid = {\n","    'C': [0.1, 1, 10],\n","    'gamma': ['scale', 'auto'],\n","    'kernel': ['linear', 'rbf', 'poly']\n","}\n","\n","# Initialize the SVM classifier\n","svm_classifier = svm.SVC()\n","\n","# Perform grid search with cross-validation\n","grid_search = GridSearchCV(estimator=svm_classifier, param_grid=param_grid, cv=5)\n","grid_search.fit(X_train, y_train)\n","\n","# Get the best-performing model and its hyperparameters\n","best_model = grid_search.best_estimator_\n","best_hyperparameters = grid_search.best_params_\n","\n","# Evaluate the best model on the test set\n","y_pred = best_model.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Best Hyperparameters:\", best_hyperparameters)\n","print(\"Accuracy:\", accuracy)\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred))\n"],"metadata":{"id":"R2UQ9RyTBHi7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.32)   Train an SVM Classifier on an imbalanced dataset and apply class weighting."],"metadata":{"id":"XuNHLvRwBaIT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn import svm\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.metrics import accuracy_score, classification_report, f1_score\n","\n","# Load the iris dataset (we'll make it imbalanced)\n","iris = datasets.load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Make the dataset imbalanced by removing some samples from one class\n","np.random.seed(0)\n","indices_to_remove = np.random.choice(np.where(y == 0)[0], size=40, replace=False)\n","y = np.delete(y, indices_to_remove)\n","X = np.delete(X, indices_to_remove, axis=0)\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Compute class weights\n","class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n","\n","# Create a dictionary mapping class labels to their corresponding weights\n","class_weight_dict = dict(enumerate(class_weights))\n","\n","# Train an SVM classifier with class weighting\n","svm_classifier = svm.SVC(class_weight=class_weight_dict, kernel='rbf', C=1)\n","svm_classifier.fit(X_train, y_train)\n","\n","# Evaluate the classifier on the test set\n","y_pred = svm_classifier.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred, average='macro')\n","print(\"Accuracy:\", accuracy)\n","print(\"F1-score (macro):\", f1)\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred))\n","\n"],"metadata":{"id":"5Jc2LhEXBaQM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.33) Implement a Naïve Bayes classifier for spam detection using email data."],"metadata":{"id":"T8FCttujBaT3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# Load the email dataset\n","data = pd.read_csv('email_data.csv')\n","\n","# Split the data into training and testing sets\n","X = data['email']\n","y = data['label']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Create a bag-of-words representation of the email data\n","vectorizer = CountVectorizer(stop_words='english')\n","X_train_count = vectorizer.fit_transform(X_train)\n","X_test_count = vectorizer.transform(X_test)\n","\n","# Train a Naïve Bayes classifier\n","nb_classifier = MultinomialNB()\n","nb_classifier.fit(X_train_count, y_train)\n","\n","# Make predictions on the test set\n","y_pred = nb_classifier.predict(X_test_count)\n","\n","# Evaluate the classifier's performance\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Accuracy:\", accuracy)\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred))\n","\n"],"metadata":{"id":"SRUmmWwrBaXc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.34) Train an SVM Classifier and a Naïve Bayes Classifier on the same dataset and compare accuracy.\n","\n"],"metadata":{"id":"pZ6-XQxtBaa6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn import svm\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# Load the iris dataset\n","iris = datasets.load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train an SVM Classifier\n","svm_classifier = svm.SVC(kernel='rbf', C=1)\n","svm_classifier.fit(X_train, y_train)\n","\n","# Train a Naïve Bayes Classifier\n","nb_classifier = MultinomialNB()\n","# Since MultinomialNB expects feature values to be non-negative, we'll use a different dataset\n","# Let's use the 20 newsgroups dataset instead\n","from sklearn.datasets import fetch_20newsgroups\n","newsgroups_train = fetch_20newsgroups(subset='train')\n","X_train_nb = newsgroups_train.data\n","y_train_nb = newsgroups_train.target\n","newsgroups_test = fetch_20newsgroups(subset='test')\n","X_test_nb = newsgroups_test.data\n","y_test_nb = newsgroups_test.target\n","\n","# We need to vectorize the text data\n","from sklearn.feature_extraction.text import CountVectorizer\n","vectorizer = CountVectorizer()\n","X_train_nb_vectorized = vectorizer.fit_transform(X_train_nb)\n","X_test_nb_vectorized = vectorizer.transform(X_test_nb)\n","\n","nb_classifier.fit(X_train_nb_vectorized, y_train_nb)\n","\n","# Make predictions using the SVM Classifier\n","y_pred_svm = svm_classifier.predict(X_test)\n","\n","# Make predictions using the Naïve Bayes Classifier\n","y_pred_nb = nb_classifier.predict(X_test_nb_vectorized)\n","\n","# Evaluate the accuracy of both classifiers\n","accuracy_svm = accuracy_score(y_test, y_pred_svm)\n","accuracy_nb = accuracy_score(y_test_nb, y_pred_nb)\n","\n","print(\"SVM Classifier Accuracy:\", accuracy_svm)\n","print(\"Naïve Bayes Classifier Accuracy:\", accuracy_nb)\n","\n","print(\"SVM Classifier Classification Report:\")\n","print(classification_report(y_test, y_pred_svm))\n","\n","print(\"Naïve Bayes Classifier Classification Report:\")\n","print(classification_report(y_test_nb, y_pred_nb))\n","\n"],"metadata":{"id":"UKJ5fks4Bahv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.35) Perform feature selection before training a Naïve Bayes classifier and compare results.\n","\n"],"metadata":{"id":"_6JOZYMuDIuj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_selection import SelectKBest, chi2\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# Load the iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Perform feature selection using chi-squared test\n","selector = SelectKBest(chi2, k=2)\n","X_train_selected = selector.fit_transform(X_train, y_train)\n","X_test_selected = selector.transform(X_test)\n","\n","# Train a Naïve Bayes classifier on the original dataset\n","nb_classifier = MultinomialNB()\n","nb_classifier.fit(X_train, y_train)\n","\n","# Train a Naïve Bayes classifier on the dataset with selected features\n","nb_classifier_selected = MultinomialNB()\n","nb_classifier_selected.fit(X_train_selected, y_train)\n","\n","# Make predictions using both classifiers\n","y_pred = nb_classifier.predict(X_test)\n","y_pred_selected = nb_classifier_selected.predict(X_test_selected)\n","\n","# Evaluate the accuracy of both classifiers\n","accuracy = accuracy_score(y_test, y_pred)\n","accuracy_selected = accuracy_score(y_test, y_pred_selected)\n","\n","print(\"Accuracy without feature selection:\", accuracy)\n","print(\"Accuracy with feature selection:\", accuracy_selected)\n","\n","print(\"Classification Report without feature selection:\")\n","print(classification_report(y_test, y_pred))\n","\n","print(\"Classification Report with feature selection:\")\n","print(classification_report(y_test, y_pred_selected))\n"],"metadata":{"id":"a8eQ29pODI27"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.36) Train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO) strategies on the Wine dataset.\n","\n"],"metadata":{"id":"8RnMTmqvDI6d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import SVC\n","from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load the Wine dataset\n","wine = datasets.load_wine()\n","X, y = wine.data, wine.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize features by scaling to unit variance\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# Train SVM using One-vs-Rest (OvR) strategy\n","svm_ovr = OneVsRestClassifier(SVC(kernel='linear', C=1.0, random_state=42))\n","svm_ovr.fit(X_train, y_train)\n","y_pred_ovr = svm_ovr.predict(X_test)\n","accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n","\n","# Train SVM using One-vs-One (OvO) strategy\n","svm_ovo = OneVsOneClassifier(SVC(kernel='linear', C=1.0, random_state=42))\n","svm_ovo.fit(X_train, y_train)\n","y_pred_ovo = svm_ovo.predict(X_test)\n","accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n","\n","# Print accuracy comparison\n","print(f'Accuracy with One-vs-Rest (OvR): {accuracy_ovr:.2f}')\n","print(f'Accuracy with One-vs-One (OvO): {accuracy_ovo:.2f}')\n"],"metadata":{"id":"4rtmdTWjDI9p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.37) Train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast Cancer dataset."],"metadata":{"id":"VzG0jPonDJAz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","\n","# Load the Breast Cancer dataset\n","cancer = datasets.load_breast_cancer()\n","X, y = cancer.data, cancer.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize features by scaling to unit variance\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# Train SVM with Linear kernel\n","svm_linear = SVC(kernel='linear', C=1.0, random_state=42)\n","svm_linear.fit(X_train, y_train)\n","y_pred_linear = svm_linear.predict(X_test)\n","accuracy_linear = accuracy_score(y_test, y_pred_linear)\n","\n","# Train SVM with Polynomial kernel\n","svm_poly = SVC(kernel='poly', degree=3, C=1.0, random_state=42)\n","svm_poly.fit(X_train, y_train)\n","y_pred_poly = svm_poly.predict(X_test)\n","accuracy_poly = accuracy_score(y_test, y_pred_poly)\n","\n","# Train SVM with RBF kernel\n","svm_rbf = SVC(kernel='rbf', C=1.0, random_state=42)\n","svm_rbf.fit(X_train, y_train)\n","y_pred_rbf = svm_rbf.predict(X_test)\n","accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n","\n","# Print accuracy comparison\n","print(f'Accuracy with Linear Kernel: {accuracy_linear:.2f}')\n","print(f'Accuracy with Polynomial Kernel: {accuracy_poly:.2f}')\n","print(f'Accuracy with RBF Kernel: {accuracy_rbf:.2f}')\n"],"metadata":{"id":"pFA93XMLDJEL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.38)  Train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the average accuracy."],"metadata":{"id":"warRNsGzDJNj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) from sklearn import datasets\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn import svm\n","from sklearn.metrics import accuracy_score\n","\n","# Load the iris dataset\n","iris = datasets.load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Initialize the Stratified K-Fold Cross-Validation object\n","kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","# Initialize the list to store the accuracy scores\n","accuracy_scores = []\n","\n","# Train an SVM Classifier using Stratified K-Fold Cross-Validation\n","for train_index, test_index in kfold.split(X, y):\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y[train_index], y[test_index]\n","\n","    # Train an SVM Classifier on the current fold\n","    svm_classifier = svm.SVC(kernel='rbf', C=1)\n","    svm_classifier.fit(X_train, y_train)\n","\n","    # Make predictions on the test set\n","    y_pred = svm_classifier.predict(X_test)\n","\n","    # Compute the accuracy score\n","    accuracy = accuracy_score(y_test, y_pred)\n","    accuracy_scores.append(accuracy)\n","\n","# Compute the average accuracy\n","average_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n","print(\"Average Accuracy:\", average_accuracy)\n","\n"],"metadata":{"id":"B_tXgJwCDnA7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.39) Write a Python program to train a Naïve Bayes classifier using different prior probabilities and compare performance."],"metadata":{"id":"ADBI5rNZDnLT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import accuracy_score, classification_report\n","import numpy as np\n","\n","# Load the iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Define different prior probabilities\n","prior_probabilities = [\n","    None,  # Default prior probabilities (uniform distribution)\n","    [0.4, 0.3, 0.3],  # Custom prior probabilities\n","    [0.6, 0.2, 0.2],  # Custom prior probabilities\n","]\n","\n","# Train a Naïve Bayes classifier using different prior probabilities\n","for prior_probability in prior_probabilities:\n","    # Initialize the Naïve Bayes classifier with the specified prior probability\n","    if prior_probability is None:\n","        nb_classifier = GaussianNB()\n","    else:\n","        nb_classifier = GaussianNB(priors=prior_probability)\n","\n","    # Train the Naïve Bayes classifier\n","    nb_classifier.fit(X_train, y_train)\n","\n","    # Make predictions on the test set\n","    y_pred = nb_classifier.predict(X_test)\n","\n","    # Evaluate the accuracy of the classifier\n","    accuracy = accuracy_score(y_test, y_pred)\n","    print(\"Prior Probability:\", prior_probability)\n","    print(\"Accuracy:\", accuracy)\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, y_pred))\n","    print()\n","\n","\n"],"metadata":{"id":"1VDIkUuDDnTU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.40)  Write a Python program to perform Recursive Feature Elimination (RFE) before training an SVM Classifier and compare accuracy."],"metadata":{"id":"rlVLsAnEDnXv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn import svm\n","from sklearn.feature_selection import RFE\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# Load the iris dataset\n","iris = datasets.load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train an SVM Classifier without RFE\n","svm_classifier = svm.SVC(kernel='rbf', C=1)\n","svm_classifier.fit(X_train, y_train)\n","\n","# Make predictions without RFE\n","y_pred = svm_classifier.predict(X_test)\n","\n","# Evaluate the accuracy without RFE\n","accuracy_without_rfe = accuracy_score(y_test, y_pred)\n","print(\"Accuracy without RFE:\", accuracy_without_rfe)\n","print(\"Classification Report without RFE:\")\n","print(classification_report(y_test, y_pred))\n","\n","# Perform Recursive Feature Elimination (RFE)\n","rfe = RFE(estimator=svm.SVC(kernel='rbf', C=1), n_features_to_select=2)\n","rfe.fit(X_train, y_train)\n","\n","# Train an SVM Classifier with RFE\n","X_train_rfe = rfe.transform(X_train)\n","X_test_rfe = rfe.transform(X_test)\n","svm_classifier_rfe = svm.SVC(kernel='rbf', C=1)\n","svm_classifier_rfe.fit(X_train_rfe, y_train)\n","\n","# Make predictions with RFE\n","y_pred_rfe = svm_classifier_rfe.predict(X_test_rfe)\n","\n","# Evaluate the accuracy with RFE\n","accuracy_with_rfe = accuracy_score(y_test, y_pred_rfe)\n","print(\"Accuracy with RFE:\", accuracy_with_rfe)\n","print(\"Classification Report with RFE:\")\n","print(classification_report(y_test, y_pred_rfe))\n","\n"],"metadata":{"id":"6ScOKnKcDncS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.41) Write a Python program to train an SVM Classifier and evaluate its performance using Precision, Recall, and F1-Score instead of accuracy."],"metadata":{"id":"dPS6nebVDngb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) import numpy as np\n","from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import SVC\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","\n","# Load the Breast Cancer dataset\n","cancer = datasets.load_breast_cancer()\n","X, y = cancer.data, cancer.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize features\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# Train SVM classifier\n","svm_clf = SVC(kernel='linear', C=1.0, random_state=42)\n","svm_clf.fit(X_train, y_train)\n","y_pred = svm_clf.predict(X_test)\n","\n","# Evaluate performance using Precision, Recall, and F1-Score\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","# Print evaluation metrics\n","print(f'Precision: {precision:.2f}')\n","print(f'Recall: {recall:.2f}')\n","print(f'F1-Score: {f1:.2f}')\n"],"metadata":{"id":"8-wV4f90Dnk8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.42)  Write a Python program to train a Naïve Bayes Classifier and evaluate its performance using Log Loss (Cross-Entropy Loss)."],"metadata":{"id":"nTw85rFsDJQ7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) import numpy as np\n","from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import log_loss\n","\n","# Load the Breast Cancer dataset\n","cancer = datasets.load_breast_cancer()\n","X, y = cancer.data, cancer.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize features\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# Train Naïve Bayes classifier\n","gnb = GaussianNB()\n","gnb.fit(X_train, y_train)\n","y_prob = gnb.predict_proba(X_test)\n","\n","# Evaluate performance using Log Loss\n","log_loss_value = log_loss(y_test, y_prob)\n","\n","# Print evaluation metric\n","print(f'Log Loss: {log_loss_value:.4f}')\n"],"metadata":{"id":"QfM34bUmFSuT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.43)  Write a Python program to train an SVM Classifier and visualize the Confusion Matrix using seaborn.\n","\n"],"metadata":{"id":"RbLc8Td5FS4j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn import svm\n","from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Load the iris dataset\n","iris = datasets.load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train an SVM Classifier\n","svm_classifier = svm.SVC(kernel='rbf', C=1)\n","svm_classifier.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","y_pred = svm_classifier.predict(X_test)\n","\n","# Evaluate the accuracy of the classifier\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Accuracy:\", accuracy)\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred))\n","\n","# Create a confusion matrix\n","conf_mat = confusion_matrix(y_test, y_pred)\n","\n","# Visualize the confusion matrix using seaborn\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(conf_mat, annot=True, cmap='Blues', fmt='d')\n","plt.xlabel(\"Predicted Labels\")\n","plt.ylabel(\"True Labels\")\n","plt.title(\"Confusion Matrix\")\n","plt.show()\n","\n"],"metadata":{"id":"zuhOg-UfFS_N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.44) Write a Python program to train an SVM Regressor (SVR) and evaluate its performance using Mean Absolute Error (MAE) instead of MSE.\n","\n"],"metadata":{"id":"Gne2YdGEFTDL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans)  import numpy as np\n","from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import SVR\n","from sklearn.metrics import mean_absolute_error\n","\n","# Load the California housing dataset\n","housing = datasets.fetch_california_housing()\n","X, y = housing.data, housing.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize features\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# Train SVR model\n","svr = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n","svr.fit(X_train, y_train)\n","y_pred = svr.predict(X_test)\n","\n","# Evaluate performance using Mean Absolute Error (MAE)\n","mae = mean_absolute_error(y_test, y_pred)\n","\n","# Print evaluation metric\n","print(f'Mean Absolute Error (MAE): {mae:.4f}')\n"],"metadata":{"id":"CUxwVgkEFTJk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.45)  Write a Python program to train a Naïve Bayes classifier and evaluate its performance using the ROC-AUC score.\n","\n"],"metadata":{"id":"8NGIOS48FcEr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import roc_auc_score, accuracy_score, classification_report\n","from sklearn.preprocessing import StandardScaler\n","\n","# Generate a sample dataset\n","X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=3, n_repeated=2, random_state=42)\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize the features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Train a Naïve Bayes classifier\n","nb_classifier = GaussianNB()\n","nb_classifier.fit(X_train_scaled, y_train)\n","\n","# Make predictions on the test set\n","y_pred = nb_classifier.predict(X_test_scaled)\n","\n","# Evaluate the classifier's performance using ROC-AUC score\n","y_pred_proba = nb_classifier.predict_proba(X_test_scaled)[:, 1]\n","roc_auc = roc_auc_score(y_test, y_pred_proba)\n","print(\"ROC-AUC Score:\", roc_auc)\n","\n","# Evaluate the classifier's performance using accuracy score and classification report\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Accuracy:\", accuracy)\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred))\n"],"metadata":{"id":"Q0vuShNUFcM0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.46) Write a Python program to train an SVM Classifier and visualize the Precision-Recall Curve."],"metadata":{"id":"ODhKWikbFcRT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans)  import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import SVC\n","from sklearn.metrics import precision_recall_curve, auc\n","\n","# Load the Breast Cancer dataset\n","cancer = datasets.load_breast_cancer()\n","X, y = cancer.data, cancer.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize features\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# Train SVM classifier\n","svm_clf = SVC(kernel='linear', C=1.0, probability=True, random_state=42)\n","svm_clf.fit(X_train, y_train)\n","y_prob = svm_clf.predict_proba(X_test)[:, 1]\n","\n","# Compute Precision-Recall curve\n","precision, recall, _ = precision_recall_curve(y_test, y_prob)\n","pr_auc = auc(recall, precision)\n","\n","# Plot Precision-Recall curve\n","plt.figure(figsize=(6, 4))\n","plt.plot(recall, precision, marker='.', label=f'PR AUC = {pr_auc:.2f}')\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-Recall Curve')\n","plt.legend()\n","plt.grid()\n","plt.show()\n","\n"],"metadata":{"id":"fQK3iXJIFcX0"},"execution_count":null,"outputs":[]}]}